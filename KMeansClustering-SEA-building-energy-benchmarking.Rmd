---
title: "KMeansClustering-SEA-building-energy-bechmarking"
author: "Christopher Lacrampe"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:

    code_folding: show
    highlight: tango
    theme: yeti
    toc: yes
    toc_depth: 5
    toc_float: yes
---

The aim of this predictive model is to be able to create a new categorical variable of clusters containing similarities from the other variables in the SEA-building-energy-benchmarking-2016 data.

# 1. Loading, cleaning, and preparing the data/libraries

### 1.1 Loading libraries and datasets

```{r}
library(gmodels) # useful library for CrossTabs/confusion matrices
library(tidyverse) # has useful libraries for cleaning and filtering data
energyDat2016 = read.csv("sea-building-energy-benchmarking/2015-building-energy-benchmarking.csv", stringsAsFactors = FALSE)
```

### 1.2 Cleaning the data

#### 1.2.1 Removing outliers
Our dataset has come prepared with an **Outlier** feature where the data controller has flagged outliers as "High" and "Low", we will remove those outliers

```{r}
energyDat2016 = tbl_df(filter(energyDat2016, Outlier == ""))
energyDat2016 = as.data.frame(energyDat2016)
```

#### 1.2.2 Create class variable from the Energy Star Score (Good = ENERGYSTARScore > 75, Bad = otherwise as per EnergyStar certification guidelines)

```{r}
StarScore = ifelse(energyDat2016$ENERGYSTARScore >= 75, "Good", "Bad")
energyDat2016$StarScore = StarScore
energyDat2016$StarScore = as.factor(energyDat2016$StarScore)
```

#### 1.2.3 Create a dataframe utilizing the variables we want to include in our model (this strips redundant variables that are identical to the others just measured in different units)

```{r}
modelDat = select(energyDat2016, StarScore, ENERGYSTARScore, YearBuilt, PrimaryPropertyType, BuildingType, Neighborhood, NumberofFloors, PropertyGFATotal, SiteEUIWN.kBtu.sf., Electricity.kBtu., NaturalGas.kBtu., GHGEmissionsIntensity.kgCO2e.ft2.)
# remove NA values
modelDat = na.omit(modelDat)
```

#### 1.2.4 Next, only select the numeric features to normalize

```{r}
numericalsDat = select(modelDat, YearBuilt, NumberofFloors, PropertyGFATotal, SiteEUIWN.kBtu.sf., Electricity.kBtu., NaturalGas.kBtu., GHGEmissionsIntensity.kgCO2e.ft2.)
```

#### 1.2.5 Normalize the numeric features

```{r}
numericalsDat = as.data.frame(lapply(numericalsDat, scale))
```

### 1.3 Training the clusters

#### 1.3.1 Set the seed and train the model with k = 3

```{r}
set.seed(2345)
energy_clusters = kmeans(numericalsDat, 3)
```

# 2. Evaluating the clustering performance

### 2.1 How to obtain the size of the kmeans() clusters? Explain

```{r}
energy_clusters$size
```

This output tells us the number of observations per cluster. E.g. cluster 1 is very large, containing about 65.6% of the observations while cluster 2 is very small (3.3%)

### 2.2 How to obtain the clusters centroids and coordinates? Explain those results

```{r}
energy_clusters$centers
```

For this output, we want to check for the highest positive value for each of the columns. E.g. for YearBuilt we see cluster 1 with the largest average value, for NumberOfFloors we see cluster 2 with the largest average value, for PropertyGFATotal wee see cluster 2 with the largest average value for all but the last variable, GHGEmissionsIntensity.kgCo2e.ft2 which has cluster 3 having the largest average variable

### 2.3 Can you identify some cluster interersts? How are those results used for marketing?

From these results, we could identify cluster 1 as being the newest built cluster, with the most energy efficient variables (smallest averages for most of the energy measures). The second cluster would be those buildings with the largest number of floors and largest total floor area, it also has the highest energy emissions for most variables with the exception of the GHGEmissionsIntensity.kgCO2e.ft2. The third cluster contains the oldest buildings, which have the fewest number of floors and are typically smaller. This building has a mixture of energy outputs with the most emissions of kgCO2 per ft2 out of the other clusters. This could be useful for marketers looking to target specific groups to improve their energy efficiency.

# 3 Improving model performance

### 3.1 How to add the clusters assignments as a new feature to the full dataset? show the result of the first 10 inputs.

```{r}
modelDat$cluster = energy_clusters$cluster
modelDat[1:10,]
```

Here we can more easily identify across the 11 features whether or not the cluster classification is appropriate for each of the first 10 observations. We can see most of the first 10 observations fall in cluster 3, which are the oldest buildings

### 3.2 Does the EnergyScore vary by cluseters? How about the ENERGYSTARScore? Explain each result

```{r}
modelDat$StarScore = as.numeric(modelDat$StarScore) - 1 # 0 = bad 1 = good
summary(modelDat$StarScore)
aggregate(data = modelDat, StarScore ~ cluster, mean)
```

Before assessing this output, we have to remember the overall proportions of good energy star scores in the dataset hovers around 51%. This is useful because we can then use that value as a relative point to compare each cluster. For example, we can see that cluster 1, the cluster that involves the newest buildings, has the lowest proportion of energyStarScores. On the other hand, clusters 2 and 3 have a higher proportion. These results indicate that there are differences in building characteristics across the energy star score bins.

```{r}
summary(modelDat$ENERGYSTARScore)
aggregate(data = modelDat, ENERGYSTARScore ~cluster, mean)
```

From this output, we can verify the results found above for the factor-classified version of ENERSTARScore, where we see the mean for the whole dataset = 68.18. In this case, we again see that cluster 1 is the smallest, with cluster 3 also below the mean. And again, we see cluster 2 having a higher-than average energy star score relative to the other groups. This could be useful for marketers who would target buildings with characteristics similar to clusters 1 and 3 for products that seek to improve the energy score of a building.

