---
title: "DecisionTree-SEA-building-energy-bechmarking"
author: "Christopher Lacrampe"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:

    code_folding: show
    highlight: tango
    theme: yeti
    toc: yes
    toc_depth: 4
    toc_float: yes
---

The aim of this predictive model is to be able to classify buildings on the basis of being able to obtain Energy Star Certification (Energy Star Score >= 75). By training a Deicision Tree, we can classify other buildings in the Seattle area that have not enrolled in the energy star program, but have access to the same variables as our training and test sets.

# 1. Loading, cleaning, and preparing the data/libraries

### 1.1 Loading libraries and datasets

```{r}
library(gmodels) # useful library for CrossTabs/confusion matrices
library(C50) # has the C5.0 method used to train the decision tree model
library(tidyverse) # has useful libraries for cleaning and filtering data
energyDat2016 = read.csv("sea-building-energy-benchmarking/2015-building-energy-benchmarking.csv", stringsAsFactors = FALSE)
```

### 1.2 Cleaning the data

#### 1.2.1 Removing outliers
Our dataset has come prepared with an **Outlier** feature where the data controller has flagged outliers as "High" and "Low", we will remove those outliers

```{r}
energyDat2016 = tbl_df(filter(energyDat2016, Outlier == ""))
energyDat2016 = as.data.frame(energyDat2016)
```

#### 1.2.2 Create class variable from the Energy Star Score (Good = ENERGYSTARScore > 75, Bad = otherwise as per EnergyStar certification guidelines)

```{r}
StarScore = ifelse(energyDat2016$ENERGYSTARScore >= 75, "Good", "Bad")
energyDat2016$StarScore = StarScore
energyDat2016$StarScore = as.factor(energyDat2016$StarScore)
```

#### 1.2.3 Create a dataframe utilizing the variables we want to include in our model (this strips redundant variables that are identical to the others just measured in different units)

```{r}
modelDat = select(energyDat2016, StarScore, YearBuilt, PropertyGFATotal, SiteEUIWN.kBtu.sf., Electricity.kBtu., NaturalGas.kBtu., GHGEmissionsIntensity.kgCO2e.ft2.)
# remove NA values
modelDat = na.omit(modelDat)
```

## 2. Splitting the data and training the model

#### 2.1 Splitting the dataset

```{r}
# randomize the entries and split into training and test
set.seed(123)
mod_rand = order(runif(2491))

# split the randomized set into a training set and test set
mod_train = modelDat[mod_rand[1:2000],]
mod_test = modelDat[mod_rand[2000:2491],]

# store the target variable as seperate vectors
mod_train_labels = as.factor(as.vector(mod_train$StarScore))
mod_test_labels = as.factor(as.vector(mod_test$StarScore))

```

#### 2.2 Training the model using C5.0 algorithm

```{r}
energy_model = C5.0(mod_train[-1], mod_train$StarScore)
energy_model
```

##### 2.2.1 Explain the results of the training model

We can see that our tree is 16 layers deep (can at worst have to compute 16 decisions), using 6 features and 2000 observation.

##### 2.2.1 Graphing the tree

```{r}
summary(energy_model)
```

From this output, we can walk through the decision tree by following the different paths (thre tree is represented left to right, with the root on the top left). For instance, from the top we can see the initial node is based on brittish thermal units per square foot of the space being greater than 62.6.  

At the bottom of the model, we can see a confusion matrix. The error rate is given as 24.5% (490 times), with 289 false positives and 201 false negatives. 

#### 2.2.3 Evaluating model performance

##### 2.2.3.1 Use the energy_model to predict on the test set

```{r}
energy_pred = predict(energy_model, mod_test)
```

##### 2.2.3.2 Use a cross-tab to evaluate the model's performance

```{r}
CrossTable(mod_test$StarScore, energy_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual', 'predicted'))
```


